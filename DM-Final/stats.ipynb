{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 读入预处理好的 csv 文件 `processed_data.csv` 并给出统计信息。<br/>\n",
    "* 将 unigrams 以及 bigrams 的频率分布写入 pickle 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tweets, num_pos_tweets, num_neu_tweets, num_neg_tweets = 0, 0, 0, 0\n",
    "num_mentions, max_mentions = 0, 0\n",
    "num_emojis, num_pos_emojis, num_neg_emojis, max_emojis = 0, 0, 0, 0\n",
    "num_urls, max_urls = 0, 0\n",
    "num_unigrams, num_unique_unigrams, min_unigrams, max_unigrams = 0, 0, 1e6, 0\n",
    "num_bigrams, num_unique_bigrams = 0, 0\n",
    "\n",
    "all_unigrams = []\n",
    "all_bigrams = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "processed_file_path = './processed_data.csv'\n",
    "all_data = pd.read_csv(processed_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tweets = len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tweet(tweet):\n",
    "    result = dict()\n",
    "    result['MENTIONS'] = tweet.count('userMentionToken')\n",
    "    result['URLS'] = tweet.count('urlToken')\n",
    "    result['POS_EMOS'] = tweet.count(':)')\n",
    "    result['NEG_EMOS'] = tweet.count(':(')\n",
    "    unigrams = word_tokenizer.tokenize(tweet)\n",
    "    result['UNIGRAMS'] = len(unigrams)\n",
    "    bigrams = get_bigrams(unigrams)\n",
    "    result['BIGRAMS'] = len(bigrams)\n",
    "    return result, unigrams, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(unigrams):\n",
    "    bigrams = []\n",
    "    for i in range(len(unigrams) - 1):\n",
    "        bigrams.append((unigrams[i], unigrams[i+1]))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_tweets):\n",
    "    row = all_data.iloc[i]\n",
    "    if row.sentiment == 'positive':\n",
    "        num_pos_tweets += 1\n",
    "    elif row.sentiment == 'neutral':\n",
    "        num_neu_tweets += 1\n",
    "    elif row.sentiment == 'negative':\n",
    "        num_neg_tweets += 1\n",
    "    result, unigrams, bigrams = analyze_tweet(row.text)\n",
    "    num_mentions += result['MENTIONS']\n",
    "    max_mentions = max(max_mentions, result['MENTIONS'])\n",
    "    num_pos_emojis += result['POS_EMOS']\n",
    "    num_neg_emojis += result['NEG_EMOS']\n",
    "    max_emojis = max(max_emojis, result['POS_EMOS'] + result['NEG_EMOS'])\n",
    "    num_urls += result['URLS']\n",
    "    max_urls = max(max_urls, result['URLS'])\n",
    "    num_unigrams += result['UNIGRAMS']\n",
    "    min_unigrams = min(min_unigrams, result['UNIGRAMS'])\n",
    "    max_unigrams = max(max_unigrams, result['UNIGRAMS'])\n",
    "    all_unigrams.extend(unigrams)\n",
    "    num_bigrams += result['BIGRAMS']\n",
    "    all_bigrams.extend(bigrams)\n",
    "\n",
    "num_emojis = num_pos_emojis + num_neg_emojis\n",
    "unique_unigrams = list(set(all_unigrams))\n",
    "with open('./unique-unigrams.txt', 'w') as uut:\n",
    "    uut.write('\\n'.join(unique_unigrams))\n",
    "num_unique_unigrams = len(unique_unigrams)\n",
    "num_unique_bigrams = len(set(all_bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nCalculating frequency distribution')\n",
    "# Unigrams\n",
    "unigrams_freq_dist = FreqDist(all_unigrams)\n",
    "with open('./unigrams-freqdist.pkl', 'wb') as uf_pkl:\n",
    "    pickle.dump(unigrams_freq_dist, uf_pkl)\n",
    "print('Saved unigrams-frequency distribution to unigrams-freqdist.pkl')\n",
    "# Bigrams\n",
    "bigrams_freq_dist = FreqDist(all_bigrams)\n",
    "with open('./bigrams-freqdist.pkl', 'wb') as bf_pkl:\n",
    "    pickle.dump(bigrams_freq_dist, bf_pkl)\n",
    "print('Saved bigrams-frequency distribution to bigrams-freqdist.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nAnalysis Statistics')\n",
    "print('Tweets => Total: {}, Positive: {}, Neutral: {}, Negative: {}'.format(num_tweets, \n",
    "                                                                            num_pos_tweets, \n",
    "                                                                            num_neu_tweets, \n",
    "                                                                            num_neg_tweets))\n",
    "print('User Mentions => Total: {}, Avg: {:.4f}, Max: {}'.format(num_mentions,\n",
    "                                                                num_mentions / float(num_tweets),\n",
    "                                                                max_mentions))\n",
    "print('URLs => Total: {}, Avg: {:.4f}, Max: {}'.format(num_urls, \n",
    "                                                       num_urls / float(num_tweets),\n",
    "                                                       max_urls))\n",
    "print('Emojis => Total: {}, Positive: {}, Negative: {}, Avg: {:.4f}, Max: {}'.format(num_emojis,\n",
    "                                                                                     num_pos_emojis,\n",
    "                                                                                     num_neg_emojis,\n",
    "                                                                                     num_emojis / float(num_tweets),\n",
    "                                                                                     max_emojis))\n",
    "print('Unigrams => Total: {}, Unique: {}, Avg: {:.4f}, Max: {}, Min: {}'.format(num_tweets,\n",
    "                                                                                num_unique_unigrams,\n",
    "                                                                                num_unigrams / float(num_tweets),\n",
    "                                                                                max_unigrams,\n",
    "                                                                                min_unigrams))\n",
    "print('Bigrams => Total: {}, Unique: {}, Avg: {:.4f}'.format(num_bigrams,\n",
    "                                                             num_unique_bigrams,\n",
    "                                                             num_bigrams / float(num_tweets)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
