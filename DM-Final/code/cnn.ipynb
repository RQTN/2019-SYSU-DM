{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, ConcatDataset\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.data import BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = '../data/processed_train.csv'\n",
    "TEST_FILE = '../data/processed_test.csv'\n",
    "\n",
    "train_data = pd.read_csv(TRAIN_FILE)\n",
    "test_data = pd.read_csv(TEST_FILE)\n",
    "word_tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = data.Field(lower=False, tokenize=word_tokenizer.tokenize, batch_first=True)\n",
    "sentiment_field = data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, text_field, sentiment_field, pd_data, **kwargs):\n",
    "        fields = [('id', None), ('sentiment', sentiment_field), ('text', text_field)]\n",
    "        \n",
    "        examples = []\n",
    "        \n",
    "        def sentimentMapping(sentiment):\n",
    "            if sentiment == 'negative':\n",
    "                return 0\n",
    "            elif sentiment == 'neutral':\n",
    "                return 1\n",
    "            elif sentiment == 'positive':\n",
    "                return 2\n",
    "            \n",
    "        for i in range(len(pd_data)):\n",
    "            e = pd_data.iloc[i]\n",
    "            examples.append(data.Example.fromlist([e['id'], sentimentMapping(e['sentiment']), e['text']], fields))\n",
    "            \n",
    "        \n",
    "        super(CustomDataset, self).__init__(examples, fields, **kwargs)\n",
    "        \n",
    "    @classmethod\n",
    "    def splits(cls, text_field, sentiment_field, pd_data_1, pd_data_2, **kwargs):\n",
    "        return (cls(text_field, sentiment_field, pd_data_1),\n",
    "               cls(text_field, sentiment_field, pd_data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = CustomDataset.splits(text_field, sentiment_field, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = '../data/vector_cache'\n",
    "\n",
    "if not os.path.exists(cache):\n",
    "    os.mkdir(cache)\n",
    "    \n",
    "glove_vectors = Vectors(name='glove.840B.300d.txt', cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myUniform(w):\n",
    "    return nn.init.uniform_(w, a=-0.25, b=0.25)\n",
    "\n",
    "glove_vectors.unk_init = myUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(train_dataset, test_dataset, vectors=glove_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataIter(dataset_1, dataset_2, sort_within_batch):\n",
    "    \n",
    "    iter_1 = BucketIterator(dataset_1, \n",
    "                                batch_size=batch_size, \n",
    "                                sort_key=lambda x: len(x.text), \n",
    "                                device=device, \n",
    "                                sort=False,\n",
    "                                sort_within_batch=sort_within_batch, \n",
    "                                repeat=False,\n",
    "                                train=True)\n",
    "\n",
    "    iter_2 = BucketIterator(dataset_2, \n",
    "                                batch_size=batch_size, \n",
    "                                sort_key=lambda x: len(x.text), \n",
    "                                device=device, \n",
    "                                sort=False,\n",
    "                                sort_within_batch=sort_within_batch, \n",
    "                                repeat=False,\n",
    "                                train=False)\n",
    "    \n",
    "    return iter_1, iter_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, text_field, kernel_num, kernel_sizes, dropout):\n",
    "        \n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        vocab_size = text_field.vocab.vectors.size()[0]\n",
    "        embed_dim = text_field.vocab.vectors.size()[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # self.embedding.weight.data.copy_(text_field.vocab.vectors)\n",
    "        \n",
    "        self.convs = nn.ModuleList(\n",
    "            nn.Conv1d(\n",
    "                in_channels=embed_dim,\n",
    "                out_channels=kernel_num,\n",
    "                kernel_size=kernel_size\n",
    "            ) for kernel_size in kernel_sizes\n",
    "\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(len(kernel_sizes) * kernel_num, 3)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        text = self.embedding(text).transpose(1, 2)\n",
    "        \n",
    "        texts = [conv(text) for conv in self.convs]\n",
    "        texts = [F.max_pool1d(t, t.size(2)).squeeze(2) for t in texts]\n",
    "        \n",
    "        text = torch.cat(texts, dim=1)\n",
    "        \n",
    "        text = self.dropout(text)\n",
    "        \n",
    "        text = self.fc(F.sigmoid(text))\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 0\n",
    "\n",
    "num_epochs = 2\n",
    "cross_val_fold = 5\n",
    "batch_size = 16\n",
    "kernel_num = 50\n",
    "kernel_sizes = [2, 2, 2, 2, 3, 3, 3]\n",
    "dropout = 0.3\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: \n",
      "Min length of text: 3\n",
      "TEST: \n",
      "Min length of text: 3\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = getDataIter(train_dataset, test_dataset, True)\n",
    "\n",
    "def find_min_len(title, iterator):\n",
    "    print(title)\n",
    "    minLenOfText = 1000\n",
    "    for i in iterator:\n",
    "        if len(i.text.size()) == 1:\n",
    "            minLenOfText = 1\n",
    "        else:\n",
    "            minLenOfText = i.text.size()[1] if i.text.size()[1] < minLenOfText else minLenOfText\n",
    "            \n",
    "    print('Min length of text: {}'.format(minLenOfText))\n",
    "    \n",
    "find_min_len('TRAIN: ', train_iter)\n",
    "find_min_len('TEST: ', test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_bias_reset(model, use_glove):\n",
    "\n",
    "    for m in clf_CNN.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            mean, std = 0, 0.01 \n",
    "            nn.init.normal_(m.weight, mean, std)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv1d):\n",
    "            mean, std = 0, 0.01 \n",
    "            nn.init.normal_(m.weight, mean, std)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            if not use_glove:\n",
    "                myUniform(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_CNN = TextCNN(text_field, kernel_num, kernel_sizes, dropout)\n",
    "clf_CNN.to(device)\n",
    "weight_bias_reset(clf_CNN, False)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(clf_CNN.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, model, loss_fn, optimizer):\n",
    "    \n",
    "    steps = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        text, sentiment = batch.text, batch.sentiment\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(text)\n",
    "        _, predicts = outputs.max(dim=1)\n",
    "        \n",
    "        loss = loss_fn(outputs, sentiment)\n",
    "        loss.backward()\n",
    "        \n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        if steps % log_interval == 0:\n",
    "            corrects = (predicts == sentiment).sum()\n",
    "            acc = 100.0 * float(corrects) / float(batch.batch_size)\n",
    "            print('\\rTrain: Batch[{}] - loss: {:.6f} acc: {:.4f}%({}/{})'.format(steps,\n",
    "                                                                                 loss.item(), \n",
    "                                                                                 acc,\n",
    "                                                                                 corrects,\n",
    "                                                                                 batch.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(val_iter, model, loss_fn):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            text, sentiment = batch.text, batch.sentiment\n",
    "            \n",
    "            outputs = model(text)\n",
    "            _, predicts = outputs.max(dim=1)\n",
    "            \n",
    "            loss = loss_fn(outputs, sentiment)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            corrects += (predicts == sentiment).sum()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_iter)\n",
    "    acc = 100.0 * float(corrects) / float(len(val_iter.dataset))\n",
    "    \n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, loss_fn, optimizer, num_epochs, cross_val_fold, sort_within_batch):\n",
    "    \n",
    "    valset_len = len(train_data) // cross_val_fold\n",
    "    splitedsets = random_split(train_data, tuple([valset_len] * (cross_val_fold - 1) + [len(train_data) - valset_len * (cross_val_fold - 1)]))\n",
    "    \n",
    "    train_accs = []\n",
    "    train_losses = []\n",
    "    \n",
    "    val_accs = []\n",
    "    val_losses = []\n",
    "    \n",
    "    test_accs = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for fid in range(cross_val_fold):\n",
    "        \n",
    "        train_indices = []\n",
    "        for i, x in enumerate(splitedsets):\n",
    "            if i != fid:\n",
    "                train_indices.extend(x.indices.numpy())\n",
    "        \n",
    "        val_indices = splitedsets[fid].indices\n",
    "        \n",
    "        train_dataset, val_dataset = CustomDataset.splits(text_field, sentiment_field, \n",
    "                                                         train_data.iloc[train_indices], train_data.iloc[val_indices])\n",
    "        \n",
    "        print(f'\\nFold:{fid}')\n",
    "        \n",
    "        train_iter, val_iter = getDataIter(train_dataset, val_dataset, sort_within_batch)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'\\rEpoch: {epoch}')\n",
    "            \n",
    "            train(train_iter, model, loss_fn, optimizer)\n",
    "            train_loss, train_acc = val(train_iter, model, loss_fn)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            \n",
    "            # --------------------------------------\n",
    "            \n",
    "            val_loss, val_acc = val(val_iter, model, loss_fn)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print('\\rEvaluation: loss: {:.6f} acc: {:.4f}%'.format(val_loss, val_acc))\n",
    "            \n",
    "            # --------------------------------------\n",
    "            \n",
    "            test_loss, test_acc = val(test_iter, model, loss_fn)\n",
    "            test_losses.append(test_loss)\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "            print('\\rTest: loss: {:.6f} acc: {:.4f}%'.format(test_loss, test_acc))\n",
    "            \n",
    "    return train_accs, train_losses, val_accs, val_losses, test_accs, test_losses\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_curve(ys, title):\n",
    "    x = np.array(range(len(ys)))\n",
    "    y = np.array(ys)\n",
    "    plt.plot(x, y, c='b')\n",
    "    plt.axis()\n",
    "    \n",
    "    plt.title('{} Curve:'.format(title))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('{} Value'.format(title))\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold:0\n",
      "Epoch: 0\n",
      "Train: Batch[100] - loss: 1.167614 acc: 37.5000%(6/16)\n",
      "Train: Batch[200] - loss: 1.065162 acc: 37.5000%(6/16)\n",
      "Train: Batch[300] - loss: 0.584525 acc: 81.2500%(13/16)\n",
      "Train: Batch[400] - loss: 1.017393 acc: 68.7500%(11/16)\n",
      "Train: Batch[500] - loss: 0.539509 acc: 87.5000%(14/16)\n",
      "Evaluation: loss: 0.572919 acc: 76.9001%\n",
      "Test: loss: 0.576430 acc: 77.1516%\n",
      "Epoch: 1\n",
      "Train: Batch[100] - loss: 0.501204 acc: 81.2500%(13/16)\n",
      "Train: Batch[200] - loss: 0.516090 acc: 81.2500%(13/16)\n",
      "Train: Batch[300] - loss: 0.520114 acc: 75.0000%(12/16)\n",
      "Train: Batch[400] - loss: 0.311539 acc: 87.5000%(14/16)\n",
      "Train: Batch[500] - loss: 0.244083 acc: 87.5000%(14/16)\n",
      "Evaluation: loss: 0.531991 acc: 79.7182%\n",
      "Test: loss: 0.553216 acc: 78.4153%\n",
      "\n",
      "Fold:1\n",
      "Epoch: 0\n",
      "Train: Batch[100] - loss: 0.352532 acc: 93.7500%(15/16)\n",
      "Train: Batch[200] - loss: 0.537588 acc: 81.2500%(13/16)\n",
      "Train: Batch[300] - loss: 0.301353 acc: 87.5000%(14/16)\n",
      "Train: Batch[400] - loss: 0.194670 acc: 93.7500%(15/16)\n",
      "Train: Batch[500] - loss: 0.389099 acc: 81.2500%(13/16)\n",
      "Evaluation: loss: 0.270797 acc: 90.3928%\n",
      "Test: loss: 0.569794 acc: 79.4057%\n",
      "Epoch: 1\n",
      "Train: Batch[100] - loss: 0.332654 acc: 87.5000%(14/16)\n",
      "Train: Batch[200] - loss: 0.120222 acc: 93.7500%(15/16)\n",
      "Train: Batch[300] - loss: 0.099591 acc: 100.0000%(16/16)\n",
      "Train: Batch[400] - loss: 0.210671 acc: 93.7500%(15/16)\n",
      "Train: Batch[500] - loss: 0.173503 acc: 93.7500%(15/16)\n",
      "Evaluation: loss: 0.300683 acc: 89.7950%\n",
      "Test: loss: 0.654023 acc: 78.4153%\n",
      "\n",
      "Fold:2\n",
      "Epoch: 0\n",
      "Train: Batch[100] - loss: 0.201486 acc: 93.7500%(15/16)\n",
      "Train: Batch[200] - loss: 0.362098 acc: 87.5000%(14/16)\n",
      "Train: Batch[300] - loss: 0.290192 acc: 81.2500%(13/16)\n",
      "Train: Batch[400] - loss: 0.301916 acc: 87.5000%(14/16)\n",
      "Train: Batch[500] - loss: 0.153540 acc: 93.7500%(15/16)\n",
      "Evaluation: loss: 0.130981 acc: 95.2605%\n",
      "Test: loss: 0.683698 acc: 78.4836%\n",
      "Epoch: 1\n",
      "Train: Batch[100] - loss: 0.112104 acc: 93.7500%(15/16)\n",
      "Train: Batch[200] - loss: 0.109410 acc: 93.7500%(15/16)\n",
      "Train: Batch[300] - loss: 0.099968 acc: 93.7500%(15/16)\n",
      "Train: Batch[400] - loss: 0.086331 acc: 93.7500%(15/16)\n",
      "Train: Batch[500] - loss: 0.032315 acc: 100.0000%(16/16)\n",
      "Evaluation: loss: 0.141852 acc: 94.5773%\n",
      "Test: loss: 0.778707 acc: 77.0833%\n",
      "\n",
      "Fold:3\n",
      "Epoch: 0\n",
      "Train: Batch[100] - loss: 0.239658 acc: 81.2500%(13/16)\n",
      "Train: Batch[200] - loss: 0.278073 acc: 93.7500%(15/16)\n",
      "Train: Batch[300] - loss: 0.007965 acc: 100.0000%(16/16)\n",
      "Train: Batch[400] - loss: 0.042775 acc: 100.0000%(16/16)\n",
      "Train: Batch[500] - loss: 0.101045 acc: 93.7500%(15/16)\n",
      "Evaluation: loss: 0.077347 acc: 97.9078%\n",
      "Test: loss: 0.887228 acc: 75.5123%\n",
      "Epoch: 1\n",
      "Train: Batch[100] - loss: 0.004458 acc: 100.0000%(16/16)\n",
      "Train: Batch[200] - loss: 0.040260 acc: 100.0000%(16/16)\n",
      "Train: Batch[300] - loss: 0.020472 acc: 100.0000%(16/16)\n",
      "Train: Batch[400] - loss: 0.054399 acc: 93.7500%(15/16)\n",
      "Train: Batch[500] - loss: 0.012449 acc: 100.0000%(16/16)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-32f9300d2aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_CNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-144-c73a897a83e9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, loss_fn, optimizer, num_epochs, cross_val_fold, sort_within_batch)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\rEpoch: {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-142-16f28112d731>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_iter, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-f6883ad9216e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-f6883ad9216e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 187\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_accs, train_losses, val_accs, val_losses, test_accs, test_losses = fit(clf_CNN, loss_fn, optimizer, num_epochs, cross_val_fold, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with Glove 0.81 ~ 0.82\n",
    "```python\n",
    "lr = 1e-3\n",
    "weight_decay = 0\n",
    "\n",
    "num_epochs = 2\n",
    "cross_val_fold = 5\n",
    "batch_size = 32\n",
    "kernel_num = 50\n",
    "kernel_sizes = [2, 2, 2, 2, 3, 3, 3]\n",
    "dropout = 0.3\n",
    "```\n",
    "without GloVe 0.79 ~ 0.795\n",
    "```python\n",
    "lr = 1e-3\n",
    "weight_decay = 0\n",
    "\n",
    "num_epochs = 2\n",
    "cross_val_fold = 5\n",
    "batch_size = 16\n",
    "kernel_num = 50\n",
    "kernel_sizes = [2, 2, 2, 2, 3, 3, 3]\n",
    "dropout = 0.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self, text_field, hidden_size, num_layers=1, dropout=0.3, bidirectional=False):\n",
    "        super(TextLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        vocab_size = text_field.vocab.vectors.size()[0]\n",
    "        embed_dim = text_field.vocab.vectors.size()[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # self.embedding.weight.data.copy_(text_field.vocab.vectors)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        \n",
    "        self.h2o = nn.Linear(self.num_directions * hidden_size, 3)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        hidden, cell = self.init_hidden(text.shape[0])\n",
    "        # text: (batch_size, seq_len)\n",
    "        text = self.embedding(text).transpose(0, 1)\n",
    "        # text: (seq_len, batch_size, embed_dim)\n",
    "        outputs, (hidden, cell) = self.lstm(text, (hidden, cell))\n",
    "\n",
    "        outputs = self.h2o(outputs[text.size(0)-1])\n",
    "        outputs = self.softmax(outputs)\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size, is_cuda=True):\n",
    "        if is_cuda:\n",
    "            cell = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "            hidden = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        else:\n",
    "            cell = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size)\n",
    "            hidden = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size)\n",
    "        \n",
    "        return hidden, cell\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-3\n",
    "weight_decay = 0\n",
    "\n",
    "num_epochs = 2\n",
    "cross_val_fold = 3\n",
    "batch_size = 16\n",
    "hidden_size = 200\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = getDataIter(train_dataset, test_dataset, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LSTM = TextLSTM(text_field, hidden_size, num_layers, dropout, bidirectional)\n",
    "clf_LSTM.to(device)\n",
    "weight_bias_reset(clf_LSTM, False)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(clf_LSTM.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold:0\n",
      "Epoch: 0\n",
      "Train: Batch[100] - loss: 0.736431 acc: 75.0000%(12/16)\n",
      "Train: Batch[200] - loss: 0.512340 acc: 81.2500%(13/16)\n",
      "Train: Batch[300] - loss: 1.263847 acc: 50.0000%(8/16)\n",
      "Train: Batch[400] - loss: 0.572216 acc: 87.5000%(14/16)\n",
      "Evaluation: loss: 0.587934 acc: 78.0994%\n",
      "Test: loss: 0.638417 acc: 75.6831%\n",
      "Epoch: 1\n",
      "Train: Batch[100] - loss: 0.256647 acc: 93.7500%(15/16)\n",
      "Train: Batch[200] - loss: 0.284155 acc: 100.0000%(16/16)\n",
      "Train: Batch[300] - loss: 0.725111 acc: 68.7500%(11/16)\n",
      "Train: Batch[400] - loss: 0.284908 acc: 93.7500%(15/16)\n",
      "Evaluation: loss: 0.528131 acc: 79.6619%\n",
      "Test: loss: 0.588631 acc: 77.1516%\n",
      "\n",
      "Fold:1\n",
      "Epoch: 0\n",
      "Train: Batch[100] - loss: 0.651106 acc: 81.2500%(13/16)\n",
      "Train: Batch[200] - loss: 0.134523 acc: 100.0000%(16/16)\n",
      "Train: Batch[300] - loss: 0.322426 acc: 87.5000%(14/16)\n",
      "Train: Batch[400] - loss: 0.319292 acc: 93.7500%(15/16)\n",
      "Evaluation: loss: 0.310634 acc: 88.6527%\n",
      "Test: loss: 0.548495 acc: 78.4495%\n",
      "Epoch: 1\n",
      "Train: Batch[100] - loss: 0.308281 acc: 93.7500%(15/16)\n",
      "Train: Batch[200] - loss: 0.209554 acc: 87.5000%(14/16)\n",
      "Train: Batch[300] - loss: 0.111532 acc: 93.7500%(15/16)\n",
      "Train: Batch[400] - loss: 0.228156 acc: 93.7500%(15/16)\n",
      "Evaluation: loss: 0.336996 acc: 87.3719%\n",
      "Test: loss: 0.665589 acc: 76.9126%\n",
      "\n",
      "Fold:2\n",
      "Epoch: 0\n",
      "Train: Batch[100] - loss: 0.418389 acc: 87.5000%(14/16)\n",
      "Train: Batch[200] - loss: 0.202986 acc: 93.7500%(15/16)\n",
      "Train: Batch[300] - loss: 0.379200 acc: 81.2500%(13/16)\n",
      "Train: Batch[400] - loss: 0.089463 acc: 100.0000%(16/16)\n",
      "Evaluation: loss: 0.159403 acc: 94.8258%\n",
      "Test: loss: 0.606917 acc: 78.2445%\n",
      "Epoch: 1\n",
      "Train: Batch[100] - loss: 0.373165 acc: 93.7500%(15/16)\n",
      "Train: Batch[200] - loss: 0.006218 acc: 100.0000%(16/16)\n",
      "Train: Batch[300] - loss: 0.177623 acc: 93.7500%(15/16)\n",
      "Train: Batch[400] - loss: 0.102408 acc: 93.7500%(15/16)\n",
      "Evaluation: loss: 0.169114 acc: 94.1855%\n",
      "Test: loss: 0.794314 acc: 78.3128%\n"
     ]
    }
   ],
   "source": [
    "train_accs, train_losses, val_accs, val_losses, test_accs, test_losses = fit(clf_LSTM, loss_fn, optimizer, num_epochs, cross_val_fold, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with GloVe 0.81 ~ 0.82\n",
    "```python\n",
    "lr = 1e-3\n",
    "weight_decay = 0\n",
    "\n",
    "num_epochs = 2\n",
    "cross_val_fold = 3\n",
    "batch_size = 32\n",
    "hidden_size = 200\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "log_interval = 100\n",
    "```\n",
    "without Glove 0.79 ~ 0.80\n",
    "```python\n",
    "lr = 2e-3\n",
    "weight_decay = 0\n",
    "\n",
    "num_epochs = 2\n",
    "cross_val_fold = 3\n",
    "batch_size = 16\n",
    "hidden_size = 200\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "log_interval = 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGRU(nn.Module):\n",
    "    def __init__(self, text_field, hidden_size, num_layers=1, dropout=0.3, bidirectional=False):\n",
    "        super(TextGRU, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        vocab_size = text_field.vocab.vectors.size()[0]\n",
    "        embed_dim = text_field.vocab.vectors.size()[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # self.embedding.weight.data.copy_(text_field.vocab.vectors)\n",
    "        \n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, num_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        \n",
    "        self.h2o = nn.Linear(self.num_directions * hidden_size, 3)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        hidden = self.init_hidden(text.shape[0])\n",
    "        # text: (batch_size, seq_len)\n",
    "        text = self.embedding(text).transpose(0, 1)\n",
    "        # text: (seq_len, batch_size, embed_dim)\n",
    "        outputs, hidden = self.gru(text, hidden)\n",
    "\n",
    "        outputs = self.h2o(outputs[text.size(0)-1])\n",
    "        outputs = self.softmax(outputs)\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size, is_cuda=True):\n",
    "        if is_cuda:\n",
    "            hidden = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        else:\n",
    "            hidden = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size)\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-3\n",
    "weight_decay = 0\n",
    "\n",
    "num_epochs = 2\n",
    "cross_val_fold = 3\n",
    "batch_size = 16\n",
    "hidden_size = 200\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = getDataIter(train_dataset, test_dataset, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_GRU = TextGRU(text_field, hidden_size, num_layers, dropout, bidirectional)\n",
    "clf_GRU.to(device)\n",
    "weight_bias_reset(clf_GRU, False)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(clf_GRU.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold:0\n",
      "Epoch: 0\n",
      "Train: Batch[100] - loss: 1.014788 acc: 56.2500%(9/16)\n",
      "Train: Batch[200] - loss: 0.604122 acc: 75.0000%(12/16)\n",
      "Train: Batch[300] - loss: 0.811116 acc: 50.0000%(8/16)\n",
      "Train: Batch[400] - loss: 0.550559 acc: 75.0000%(12/16)\n",
      "Evaluation: loss: 0.548621 acc: 78.7141%\n",
      "Test: loss: 0.573604 acc: 78.1762%\n",
      "Epoch: 1\n",
      "Train: Batch[100] - loss: 0.402460 acc: 81.2500%(13/16)\n",
      "Train: Batch[200] - loss: 0.213041 acc: 87.5000%(14/16)\n",
      "Train: Batch[300] - loss: 0.536091 acc: 81.2500%(13/16)\n",
      "Train: Batch[400] - loss: 0.569178 acc: 87.5000%(14/16)\n",
      "Evaluation: loss: 0.546827 acc: 79.7643%\n",
      "Test: loss: 0.599196 acc: 77.6639%\n",
      "\n",
      "Fold:1\n",
      "Epoch: 0\n",
      "Train: Batch[100] - loss: 0.666935 acc: 75.0000%(12/16)\n",
      "Train: Batch[200] - loss: 0.399074 acc: 87.5000%(14/16)\n",
      "Train: Batch[300] - loss: 0.659347 acc: 62.5000%(10/16)\n",
      "Train: Batch[400] - loss: 0.122752 acc: 93.7500%(15/16)\n",
      "Evaluation: loss: 0.278966 acc: 90.2408%\n",
      "Test: loss: 0.542294 acc: 79.2691%\n",
      "Epoch: 1\n",
      "Train: Batch[100] - loss: 0.207673 acc: 93.7500%(15/16)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-366-442d60170ac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_GRU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-144-c73a897a83e9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, loss_fn, optimizer, num_epochs, cross_val_fold, sort_within_batch)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\rEpoch: {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-142-16f28112d731>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_iter, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# nn.utils.clip_grad_norm_(model.parameters(), 1.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_accs, train_losses, val_accs, val_losses, test_accs, test_losses = fit(clf_GRU, loss_fn, optimizer, num_epochs, cross_val_fold, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with GloVe 0.82\n",
    "```python\n",
    "lr = 1e-3\n",
    "weight_decay = 0\n",
    "\n",
    "num_epochs = 2\n",
    "cross_val_fold = 3\n",
    "batch_size = 32\n",
    "hidden_size = 200\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "log_interval = 100\n",
    "```\n",
    "without Glove 0.79 ~ 0.80\n",
    "```python\n",
    "lr = 2e-3\n",
    "weight_decay = 0\n",
    "\n",
    "num_epochs = 2\n",
    "cross_val_fold = 3\n",
    "batch_size = 16\n",
    "hidden_size = 200\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "log_interval = 100\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
