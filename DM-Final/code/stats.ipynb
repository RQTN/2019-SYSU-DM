{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 读入预处理好的 csv 文件 `processed_data.csv` 并给出统计信息。<br/>\n",
    "* 将 unigrams 以及 bigrams 的频率分布写入 pickle 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tweets, num_pos_tweets, num_neu_tweets, num_neg_tweets = 0, 0, 0, 0\n",
    "num_mentions, max_mentions = 0, 0\n",
    "num_emojis, num_pos_emojis, num_neg_emojis, max_emojis = 0, 0, 0, 0\n",
    "num_urls, max_urls = 0, 0\n",
    "num_unigrams, num_unique_unigrams, min_unigrams, max_unigrams = 0, 0, 1e6, 0\n",
    "num_bigrams, num_unique_bigrams = 0, 0\n",
    "\n",
    "all_unigrams = []\n",
    "all_bigrams = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "processed_file_path = '../data/processed_all.csv'\n",
    "all_data = pd.read_csv(processed_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tweets = len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tweet(tweet):\n",
    "    result = dict()\n",
    "    result['MENTIONS'] = tweet.count('userMentionToken')\n",
    "    result['URLS'] = tweet.count('urlToken')\n",
    "    result['POS_EMOS'] = tweet.count(':)')\n",
    "    result['NEG_EMOS'] = tweet.count(':(')\n",
    "    unigrams = word_tokenizer.tokenize(tweet)\n",
    "    result['UNIGRAMS'] = len(unigrams)\n",
    "    bigrams = get_bigrams(unigrams)\n",
    "    result['BIGRAMS'] = len(bigrams)\n",
    "    return result, unigrams, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(unigrams):\n",
    "    bigrams = []\n",
    "    for i in range(len(unigrams) - 1):\n",
    "        bigrams.append((unigrams[i], unigrams[i+1]))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_tweets):\n",
    "    row = all_data.iloc[i]\n",
    "    if row.sentiment == 'positive':\n",
    "        num_pos_tweets += 1\n",
    "    elif row.sentiment == 'neutral':\n",
    "        num_neu_tweets += 1\n",
    "    elif row.sentiment == 'negative':\n",
    "        num_neg_tweets += 1\n",
    "    result, unigrams, bigrams = analyze_tweet(row.text)\n",
    "    num_mentions += result['MENTIONS']\n",
    "    max_mentions = max(max_mentions, result['MENTIONS'])\n",
    "    num_pos_emojis += result['POS_EMOS']\n",
    "    num_neg_emojis += result['NEG_EMOS']\n",
    "    max_emojis = max(max_emojis, result['POS_EMOS'] + result['NEG_EMOS'])\n",
    "    num_urls += result['URLS']\n",
    "    max_urls = max(max_urls, result['URLS'])\n",
    "    num_unigrams += result['UNIGRAMS']\n",
    "    min_unigrams = min(min_unigrams, result['UNIGRAMS'])\n",
    "    max_unigrams = max(max_unigrams, result['UNIGRAMS'])\n",
    "    all_unigrams.extend(unigrams)\n",
    "    num_bigrams += result['BIGRAMS']\n",
    "    all_bigrams.extend(bigrams)\n",
    "\n",
    "num_emojis = num_pos_emojis + num_neg_emojis\n",
    "unique_unigrams = list(set(all_unigrams))\n",
    "num_unique_unigrams = len(unique_unigrams)\n",
    "num_unique_bigrams = len(set(all_bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis Statistics\n",
      "Tweets => Total: 14640, Positive: 2363, Neutral: 3099, Negative: 9178\n",
      "User Mentions => Total: 16505, Avg: 1.1274, Max: 6\n",
      "URLs => Total: 1211, Avg: 0.0827, Max: 3\n",
      "Emojis => Total: 351, Positive: 249, Negative: 102, Avg: 0.0240, Max: 3\n",
      "Unigrams => Total: 14640, Unique: 13308, Avg: 20.8547, Max: 47, Min: 2\n",
      "Bigrams => Total: 290673, Unique: 100890, Avg: 19.8547\n"
     ]
    }
   ],
   "source": [
    "print('\\nAnalysis Statistics')\n",
    "print('Tweets => Total: {}, Positive: {}, Neutral: {}, Negative: {}'.format(num_tweets, \n",
    "                                                                            num_pos_tweets, \n",
    "                                                                            num_neu_tweets, \n",
    "                                                                            num_neg_tweets))\n",
    "print('User Mentions => Total: {}, Avg: {:.4f}, Max: {}'.format(num_mentions,\n",
    "                                                                num_mentions / float(num_tweets),\n",
    "                                                                max_mentions))\n",
    "print('URLs => Total: {}, Avg: {:.4f}, Max: {}'.format(num_urls, \n",
    "                                                       num_urls / float(num_tweets),\n",
    "                                                       max_urls))\n",
    "print('Emojis => Total: {}, Positive: {}, Negative: {}, Avg: {:.4f}, Max: {}'.format(num_emojis,\n",
    "                                                                                     num_pos_emojis,\n",
    "                                                                                     num_neg_emojis,\n",
    "                                                                                     num_emojis / float(num_tweets),\n",
    "                                                                                     max_emojis))\n",
    "print('Unigrams => Total: {}, Unique: {}, Avg: {:.4f}, Max: {}, Min: {}'.format(num_tweets,\n",
    "                                                                                num_unique_unigrams,\n",
    "                                                                                num_unigrams / float(num_tweets),\n",
    "                                                                                max_unigrams,\n",
    "                                                                                min_unigrams))\n",
    "print('Bigrams => Total: {}, Unique: {}, Avg: {:.4f}'.format(num_bigrams,\n",
    "                                                             num_unique_bigrams,\n",
    "                                                             num_bigrams / float(num_tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整合到一个 `.py` 文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def get_bigrams(unigrams):\n",
    "    bigrams = []\n",
    "    for i in range(len(unigrams) - 1):\n",
    "        bigrams.append((unigrams[i], unigrams[i+1]))\n",
    "    return bigrams\n",
    "\n",
    "def analyze_tweet(tweet):\n",
    "    result = dict()\n",
    "    result['MENTIONS'] = tweet.count('userMentionToken')\n",
    "    result['URLS'] = tweet.count('urlToken')\n",
    "    result['POS_EMOS'] = tweet.count(':)')\n",
    "    result['NEG_EMOS'] = tweet.count(':(')\n",
    "    unigrams = word_tokenizer.tokenize(tweet)\n",
    "    result['UNIGRAMS'] = len(unigrams)\n",
    "    bigrams = get_bigrams(unigrams)\n",
    "    result['BIGRAMS'] = len(bigrams)\n",
    "    return result, unigrams, bigrams\n",
    "\n",
    "def displayStatsInfo(file_path):\n",
    "    \n",
    "    processed_file_path = file_path\n",
    "    all_data = pd.read_csv(processed_file_path)\n",
    "\n",
    "    num_tweets, num_pos_tweets, num_neu_tweets, num_neg_tweets = 0, 0, 0, 0\n",
    "    num_mentions, max_mentions = 0, 0\n",
    "    num_emojis, num_pos_emojis, num_neg_emojis, max_emojis = 0, 0, 0, 0\n",
    "    num_urls, max_urls = 0, 0\n",
    "    num_unigrams, num_unique_unigrams, min_unigrams, max_unigrams = 0, 0, 1e6, 0\n",
    "    num_bigrams, num_unique_bigrams = 0, 0\n",
    "\n",
    "    all_unigrams = []\n",
    "    all_bigrams = []\n",
    "    \n",
    "    num_tweets = len(all_data)\n",
    "    \n",
    "    for i in range(num_tweets):\n",
    "        row = all_data.iloc[i]\n",
    "        if row.sentiment == 'positive':\n",
    "            num_pos_tweets += 1\n",
    "        elif row.sentiment == 'neutral':\n",
    "            num_neu_tweets += 1\n",
    "        elif row.sentiment == 'negative':\n",
    "            num_neg_tweets += 1\n",
    "        result, unigrams, bigrams = analyze_tweet(row.text)\n",
    "        num_mentions += result['MENTIONS']\n",
    "        max_mentions = max(max_mentions, result['MENTIONS'])\n",
    "        num_pos_emojis += result['POS_EMOS']\n",
    "        num_neg_emojis += result['NEG_EMOS']\n",
    "        max_emojis = max(max_emojis, result['POS_EMOS'] + result['NEG_EMOS'])\n",
    "        num_urls += result['URLS']\n",
    "        max_urls = max(max_urls, result['URLS'])\n",
    "        num_unigrams += result['UNIGRAMS']\n",
    "        min_unigrams = min(min_unigrams, result['UNIGRAMS'])\n",
    "        max_unigrams = max(max_unigrams, result['UNIGRAMS'])\n",
    "        all_unigrams.extend(unigrams)\n",
    "        num_bigrams += result['BIGRAMS']\n",
    "        all_bigrams.extend(bigrams)\n",
    "\n",
    "    num_emojis = num_pos_emojis + num_neg_emojis\n",
    "    unique_unigrams = list(set(all_unigrams))\n",
    "    num_unique_unigrams = len(unique_unigrams)\n",
    "    num_unique_bigrams = len(set(all_bigrams))\n",
    "    \n",
    "    print('\\nAnalysis Statistics')\n",
    "    print('Tweets => Total: {}, Positive: {}, Neutral: {}, Negative: {}'.format(num_tweets, \n",
    "                                                                                num_pos_tweets, \n",
    "                                                                                num_neu_tweets, \n",
    "                                                                                num_neg_tweets))\n",
    "    print('User Mentions => Total: {}, Avg: {:.4f}, Max: {}'.format(num_mentions,\n",
    "                                                                    num_mentions / float(num_tweets),\n",
    "                                                                    max_mentions))\n",
    "    print('URLs => Total: {}, Avg: {:.4f}, Max: {}'.format(num_urls, \n",
    "                                                           num_urls / float(num_tweets),\n",
    "                                                           max_urls))\n",
    "    print('Emojis => Total: {}, Positive: {}, Negative: {}, Avg: {:.4f}, Max: {}'.format(num_emojis,\n",
    "                                                                                         num_pos_emojis,\n",
    "                                                                                         num_neg_emojis,\n",
    "                                                                                         num_emojis / float(num_tweets),\n",
    "                                                                                         max_emojis))\n",
    "    print('Unigrams => Total: {}, Unique: {}, Avg: {:.4f}, Max: {}, Min: {}'.format(num_tweets,\n",
    "                                                                                    num_unique_unigrams,\n",
    "                                                                                    num_unigrams / float(num_tweets),\n",
    "                                                                                    max_unigrams,\n",
    "                                                                                    min_unigrams))\n",
    "    print('Bigrams => Total: {}, Unique: {}, Avg: {:.4f}'.format(num_bigrams,\n",
    "                                                                 num_unique_bigrams,\n",
    "                                                                 num_bigrams / float(num_tweets)))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
